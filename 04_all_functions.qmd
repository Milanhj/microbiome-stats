---
title: "Microbiome Functions Summary"
author: "Milan Hilde-Jones"
date: "January 24, 2025"
format:
  html:
    
    toc: true
    embed-resources: true
    code-fold: true
    link-external-newwindow: true
execute:
  
  warning: false
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: load-data-pckgs
#| eval: false

# Load Packages here
library(tidyverse)
library(DESeq2)
library(vegan)
library(patchwork)
library(kableExtra)

# Load cleaned and joined data here
load(here::here("./data/clean_data/orig_counts.rda"))
load(here::here("./data/clean_data/normalized_counts.rda"))

# Mutate, stratify, rename, etc.. data here

# Store any objects here


```


# DESeq2

`do_deseq(dat, stop_col, formula, alpha = 0.05, test = "Wald", sf_type = "custom", total_counts = NULL, ordered = FALSE, reduced = NULL)`


```{r}
#| label: do-deseq

# Raw output from DESeq2 package 
#> dat: dataframe with counts and metadata for design formula
  #> contains atleast IDs, grouping variable, and counts with all integer values
  #> Need id variable in first column
#> formula: design formula with condition of interest last
#> stop_col: index for final column of metadata
#> alpha: significance cutoff used for optimizing the independent filtering
  #> default = 0.05 here, but default is 0.1 for the package function
  #> set to adjusted p-value cutoff
#> test: (two options) Wald for cross-sectional, LRT for longitudinal
#> sf_type: how to handle size factors
  #> sf_type = "median_ratio" uses default deseq2 esimation
  #> sf_type = "custom" uses custom size factors in sf argument
#> Optional arguments:
  #> total_counts: optional vector of total counts for custom size factor estimation
  #> reduced: for LRT, formula without the final interaction term
  #> ordered: is one of the covariates ordered? if yes, set to TRUE

do_deseq <- function(dat, stop_col, formula, alpha = 0.05, test = "Wald",
                     sf_type, total_counts = NULL,
                     ordered = FALSE, reduced = NULL){
  
  # Rename id column id if it isn't already
  if(colnames(dat)[1] != "id") colnames(dat)[1] <- "id"
  
  # If any IDs are duplicated, ID with unique row numbers
  if (any(duplicated(dat$id) == TRUE)){
    # Replace old ID column with row numbers
    dat <- dat %>% 
      # remove original id
      select(-id) %>% 
      # create new id variable with unique identifiers
      mutate(
        id = row_number(),        # create ID
        id = factor(id)           # make it a factor
      ) %>% 
      # make sure new id is the first column
      relocate(id)
  } # end id if
  
  if (test == "Wald"){
    
    # Transposed Count Data Frame 
    count_data <- t(
      dat[,-c(2:stop_col)] %>% 
        column_to_rownames(var = colnames(dat[,1]))
    ) 
    
    # Column data matrix
    col_data <- dat[, 1:stop_col] %>% 
      column_to_rownames(var = colnames(dat[,1]))
    
    # Make sure col_data and count_data match
    if (unique(rownames(col_data) != colnames(count_data))) break
    
    if (ordered){ 
      # Manually make the design matrix to include ordered factors
      design_matrix <- model.matrix(formula, data = col_data)
      
      # Create DESeq Data Object
      dds <- DESeqDataSetFromMatrix(
        countData = count_data,
        colData = col_data,
        design = design_matrix # group
      )
    } else{
      
      # Create DESeq Data Object
      dds <- DESeqDataSetFromMatrix(
        countData = count_data,
        colData = col_data,
        design = formula # group
      )
    }
    
    if (sf_type == "median_ratio"){
      
      # Store Model object
      deseq_mod <- DESeq(dds, test = test)
      
    } else{
      
      if(sf_type == "custom"){
        
        # Calculate the geometric mean of total counts
        geo_mn <- exp(mean(log(total_counts))) 
        # Custom size factors
        custom_sf <- total_counts/geo_mn
        # Manually set size factors
        sizeFactors(dds) <- custom_sf 
        
        # Store Model object using 
        deseq_mod <- DESeq(dds, 
                           fitType = "parametric",
                           test = test)
      } else{ 
        stop("invalid sf_type input") 
      } # ifelse 2
      
    } # if 1
    
    # Store Results
    mod_results <- results(deseq_mod, alpha = alpha)
    
  } else {
    
    if (test == "LRT"){
      
      # Transposed Count Data Frame 
      count_data <- t(
        dat[,-c(2:stop_col)] %>% 
          column_to_rownames(var = colnames(dat[,1]))
      ) 
      
      # Column data matrix
      col_data <- dat[, c(1:stop_col)] %>% 
        column_to_rownames(var = colnames(dat[,1]))
      
      # Make sure col_data and count_data match
      if (unique(rownames(col_data) != colnames(count_data))) break
      
      # Create DESeq Data Object
      dds <- DESeqDataSetFromMatrix(
        countData = count_data,
        colData = col_data,
        # Full model
        design = formula # ~ group + timepoint + group:timepoint
      )
      
      # Store Model object with reduced model
      deseq_mod <- DESeq(dds, test = test, reduced = reduced) # ~group + timepoint
      
      # Store Results
      mod_results <- results(deseq_mod, alpha = alpha)
      
    } else{ stop("Invalid test input") } # end else 2
    
  } # end else 1
  
  return(mod_results)
  
} # End function


```



`display_deseq_results(mod, vars = NULL, var_label = NULL, digits = NULL, p.adjust.method = NULL, na.rm = FALSE)`

```{r}
#| label: display-deseq-results

# Display DESeq output in a cleaner, more easy to work with way
  #> mod: model output from my do_deseq() function or package results(DESeq())
  #> vars: optional, the names of which variables to include in the output table
  #> var_label: optional, what to name the column containing vars as rows
  #> digits: optional, how many digits to round to
  #> p.adjust.method: optional, whether or not to correct for multiple comparisons
  #> na.rm: optional, if TRUE, NA rows are removed

display_deseq_results <- function(mod, vars = NULL, var_label = NULL, digits = NULL,
                                  p.adjust.method = NULL, na.rm = FALSE){
  # Start with a generic variable name for levels
  default_label <- "outcome"
  
  # Matrix to store Values
  pvals <- matrix(
    nrow = length(rownames(mod)),
    ncol = 4,
    dimnames = list(NULL, c(default_label, "log2foldchange", "lfcSE", "p_value")))
  
  for (i in 1:length(rownames(mod))){
    # Pull out rowname from deseq output to store dat in matrix
    row_name <- rownames(mod)[i]
    # Store values
    pvals[i, 1] <- row_name
    pvals[i, 2] <- as.numeric(mod$log2FoldChange[i])
    pvals[i, 3] <- as.numeric(mod$lfcSE[i])
    pvals[i, 4] <- as.numeric(mod$pvalue[i])
    
  } # end for i
  
  # Deal with variable level display
  if (!is.null(vars)){ # select just levels included in vars
    
    # Create a temporary tibble for pulling out the variables you want
    # Extra steps so any length of vars can be used
    temp_tib <- as_tibble(pvals)
    
    # Optional padjust for multiple comparisons
    if (!is.null(p.adjust.method)) {
      temp_tib <- temp_tib %>% 
        mutate(p.adjust.method = p.adjust(p_value, method = p.adjust.method) 
        )
    }
    # Filter for just the variables listed in vars
    pvals_tibble <- tibble(
      outcome = temp_tib$outcome[temp_tib$outcome %in% vars]
    ) %>% 
      left_join(temp_tib, by = "outcome")
    
  } else{
    # Make pvals into tibble without filtering
    pvals_tibble <- as_tibble(pvals)
    
    # Optional padjust for multiple comparisons
    if (!is.null(p.adjust.method)) {
      pvals_tibble <- pvals_tibble %>% 
        mutate(p.adjust.method = p.adjust(p_value, method = p.adjust.method) 
        )
    }
  } # end ifelse
  
  # Get variables into the format we want them in
  pvals_tibble <- pvals_tibble %>% 
    mutate(
      p_value = as.numeric(p_value),
      lfcSE = as.numeric(lfcSE),
      log2foldchange = as.numeric(log2foldchange),
      foldchange = 2^log2foldchange
    ) %>% 
    relocate(foldchange, .after = log2foldchange) %>%
    arrange(default_label)
  
  # Rename outcome if a variable label is given
  if (!is.null(var_label)){
    colnames(pvals_tibble)[colnames(pvals_tibble) == "outcome"] <- var_label
  }
  # Round if digits provided
  if (!is.null(digits)){
    pvals_tibble <- pvals_tibble %>% 
      mutate(
        p_value = round(p_value, digits = digits),
        lfcSE = round(lfcSE, digits = digits),
        log2foldchange = round(log2foldchange, digits = digits),
        foldchange = round(foldchange, digits))
    # If padjust in there too, round that as well
    if(!is.null(p.adjust.method)){
      pvals_tibble <- pvals_tibble %>% 
        mutate(p.adjust.method = round(p.adjust.method, digits = digits))
    }
  } # end if
  
  return(pvals_tibble)
  
} # end function


```



`fit_deseq2(dat, stop_col, formulas, alpha = 0.05, test = "Wald", sf_type = "custom", total_counts = NULL, ordered = FALSE, reduced = NULL, na.rm = FALSE, p.adjust.method = NULL, vars = NULL ,var_label = NULL, digits = NULL)`


```{r}
#| label: fit-deseq2

# fit_deseq2(): do_deseq and display_deseq_results wrapped together
# outputs a list of tables for each variable level (gene) with results for each independent variable 
# Use this function to output results for each variable in a design formula with multiple independent variables
fit_deseq2 <- function(dat, stop_col, formulas, vars = NULL,
                       sf_type = "custom", total_counts = NULL, 
                       p.adjust.method = NULL, list = TRUE,
                       alpha = 0.05, test = "Wald",
                       ordered = FALSE, reduced = NULL, na.rm = FALSE,
                       var_label = NULL, digits = NULL){
  
  formula_outs <- list()
  # Calculate the raw package ouput 
  for (i in 1:length(formulas)){
    # Get the raw DESeq2 output
    raw_out <- do_deseq(dat, stop_col = stop_col, sf_type = sf_type, 
                        total_counts = total_counts, ordered = ordered,
                        formula = formulas[[i]][[1]], test = test) 
    
    # Organize raw output into a easier table
    out <- display_deseq_results(
      raw_out, vars = vars, var_label, digits = digits, 
      p.adjust.method = p.adjust.method, na.rm = na.rm
    )
    formula_outs[[i]] <- out
    # Add the name of the variable to the list
    if (length(formulas) == 1){
      # Pattern for splitting a formula with just 1 covariate
      covs <- str_split_i(
        as.character(formulas[[i]]), pattern = "~", 2
      )
    } else{
      # Pattern for splitting a formula with 2+ covariates
      covs <- str_split_1(
        as.character(formulas[[i]]), pattern = "[\\s]*\\+[\\s]*"
      )
    }
    
    names(formula_outs)[[i]] <- covs[[length(covs)]]
  } # end for i
  
  # List to hold tables with all results for each level
  list_results <- vector(length = nrow(out), "list")
  names(list_results) <- out[,1][[1]]
  
  for (i in 1:nrow(out)){
    # matrix to store the data with variable names in the first column
    store_dat <-  cbind(var = names(formula_outs), 
                        matrix(nrow = length(formula_outs),
                               ncol = ncol(formula_outs[[1]]),
                               dimnames = list(NULL, names(formula_outs[[1]])))
    ) # 5 cols
    for(j in 1:length(formula_outs)){
      # Fill matrix with data for a particular var level
      store_dat[j,2] <- formula_outs[[j]][i,1][[1]] # [i,1]
      store_dat[j,3] <- formula_outs[[j]][i,2][[1]] # [i,1]
      store_dat[j,4] <- formula_outs[[j]][i,3][[1]] # [i,1]
      store_dat[j,5] <- formula_outs[[j]][i,4][[1]] # [i,1]
      store_dat[j,6] <- formula_outs[[j]][i,5][[1]] # [i,1]
      
      if (!is.null(p.adjust.method)){
        store_dat[j,7] <- formula_outs[[j]][i,6][[1]]
      }
      
    } # end for j
    
    # Make outputs numeric
    final_tib <- as_tibble(store_dat) %>% 
      mutate(log2foldchange = as.numeric(log2foldchange),
             foldchange = as.numeric(foldchange),
             lfcSE = as.numeric(lfcSE),
             p_value = as.numeric(p_value)
      )
    # If padjust used, make that column numeric too
    if (!is.null(p.adjust.method)){
      final_tib <- final_tib %>% 
        mutate(p.adjust.method = as.numeric(p.adjust.method))
    } 
    
    # Fill slot for each level
    list_results[[i]] <- final_tib
  } # end for i
  
  # Return either a list or a full table
  if (list){
    return(list_results)
  } else{
    # Also deal with just single value in vars
    if(!is.null(vars) & length(vars) <= 1){
      return(list_results)
    } else{
      # Put into a single table
      tib <- list_results[[1]]
      for (j in 2:length(list_results)){
        tib <- rbind(tib, list_results[[j]])
      } # end j
      return(tib)
    }
  } # end else
  
  
} # end function
```



<br>

# T-Tests

`ttest_log2rm(dat, group_col, tot_reads_col, start_col, time_col, list = FALSE, log2 = TRUE)`
```{r}
#| label: base-ttest-log2rm-function

# start_col: the index of first column with counts
# group_col: grouping column (tx)
# total_reads_col: column with total non-human read numbers for log transforming
# time_col: time (longitudinal) or measurment month/year
  # if not a longitudinal study, use seperate datasets for pre.post intervention min-p calculation
# log2: should the data be log2+e transformed? log2(MLS + 1/total_reads*1000000)
# list: do you want the results to be returned in list format?
ttest_log2rm <- function(dat, group_col, tot_reads_col, start_col, time_col,
                         list = FALSE, log2 = TRUE){
  
  # Store variable (e.g. drug class names)
  rm_names <- colnames(dat)[start_col:length(colnames(dat))]
  
  # Change column names to generic names
  colnames(dat)[tot_reads_col] <- "total_reads"
  colnames(dat)[group_col] <- "group"
  colnames(dat)[time_col] <- "timepoint"
  
  if(list){ # Store in a list
    result <- list()
  }else{ # Store in a single table
    result <- tibble(timepoint = NA, var = NA, pval = NA)
  }
  # Loop for class
  for (i in 1:length(rm_names)){ # t-test for each time for class i
    
    # The index for class i
    dat_col <- which(names(dat) == rm_names[i]) # i
    
    # Subset data and log transform class i to create variable log2
    dat_log2 <- dat[, c(time_col, group_col, tot_reads_col, dat_col)] %>% 
      mutate(log2 = log2(!!sym(rm_names[i]) + 1/total_reads*1000000)) %>% 
      # Also give the original variable a generic name
      dplyr::rename(untrans = !!sym(rm_names[i]))
    
    # Store vector of times for class i
    times <- sort(unique(dat_log2$timepoint))
    
    # Create a matrix to hold t-test results for each timepoint
    mttest <- matrix(ncol = 3, nrow = length(times),
                     dimnames = list(NULL, c("timepoint", "var", "pval")))
    # Loop for time
    for (j in 1:length(times)){
      # Filter for just the correct timepoint
      dat_tp <- dat_log2 %>% 
        filter(timepoint == times[j])
      # Store time and class name
      mttest[j,1] <- times[j]
      mttest[j,2] <- rm_names[i]
      # T-test with either the original variable or the log2 transformed
      if (log2){ # log2 
        mttest[j,3] <- t.test(log2 ~ group, data = dat_tp)$p.value
      } else{ # raw data
        mttest[j,3] <- t.test(untrans ~ group, data = dat_tp)$p.value
      }
    } # j
    
    # Store Results
    if(list){ # Store in a list
      result[[i]] <- as_tibble(mttest) %>% 
        mutate(
          timepoint = as.numeric(timepoint),
          pval = as.numeric(pval)
        )
    }else{ # Store in a single table
      # Bind results from class i to the overall result table
      result <- rbind(result,
                      as_tibble(mttest) %>% 
                        mutate(
                          timepoint = as.numeric(timepoint),
                          pval = as.numeric(pval)
                        ))
      # If it's the first iteration, take off the top row with NA used to initialize
      if(i == 1){result <- result[-1,]}
    }
  } # i
  return(result) 
} # end function

```


`ttest_min_p(dat, B = 999, group_col, tot_reads_col, start_col, time_col, list = FALSE, log2 = TRUE)`

```{r}
#| label: minp-ttest-log2rm-function

# Runs ttest_log2rm() and Min-p for multiple comparisons
# Same output as ttest_log2rm() but with adjusted p-value
  #> B: number of permutations
  #> start_col: the index of first column with counts
  #> group_col: grouping column (tx)
  #> total_reads_col: column with total non-human read numbers for log transforming
  #> time_col: time (longitudinal) or measurment month/year
    #> if not a longitudinal study, use seperate datasets for pre.post intervention min-p calculation
  #> log2: should the data be log2+e transformed? log2(MLS + 1/total_reads*1000000)
  #> list: do you want the results to be returned in list format?
ttest_min_p <- function(dat, B = 999, group_col, tot_reads_col, 
                        start_col, time_col, 
                        list = FALSE, log2 = TRUE){
  
  # Make sure group column is named tx
  colnames(dat)[group_col] <- "tx"
  
  # Sort and store timepoints
  times <- sort(as.numeric(unique(as.vector(unlist(dat[,time_col])))))
  
  # Vector of class names
  rM_names <- colnames(dat)[c(start_col:ncol(dat))]
  
  # Call function to compute observed pvalues
  observed_result_table <- ttest_log2rm(
    dat = dat, group_col = group_col, 
    tot_reads_col = tot_reads_col, 
    start_col = start_col, time_col = time_col,
    list = list, log2 = log2
  )
  # Store observed P-values
  observed_p_values <- observed_result_table$pval
  # Extract observed minimum P-value
  observed_min_p <- min(observed_p_values, na.rm = TRUE)
  
  # Storage for permuted minimum P-values (numeric vector)
  min_perm_p_values <- numeric(B)
  
  for (b in 1:B) { # Permutation loop
    
    # Sample IDs without replacement
    perm_dat <- dat %>% 
      mutate(tx = sample(tx, length(tx), replace = FALSE))
    
    # Compute p-values from permuted data
    perm_p_values <- ttest_log2rm(
      dat = perm_dat, group_col = group_col, 
      tot_reads_col = tot_reads_col, 
      start_col = start_col, time_col = time_col,
      list = list, log2 = log2
    )
    # Store minimum P-value from vector of permuted p-values
    min_perm_p_values[b] <- min(perm_p_values$pval, na.rm = TRUE)
    
  } # for b
  
  # Tibble to hold adjusted p-values
  m_padj <- tibble(
    timepoint = rep(times, length(rM_names)),
    var = observed_result_table$var,
    minp_adj = rep(0)
  )
  # Adjust P-values
  for (i in 1:length(observed_p_values)) {
    # Proportion of permuted min p values less than or equal to the observed p-value i
    m_padj[i,3] <- mean(min_perm_p_values <= observed_p_values[i])
    
  } # for i
  
  # Join to raw p-value from observed results
  padj_result_table <- observed_result_table %>% 
    left_join(m_padj, by = c("var", "timepoint"))
  
  return(padj_result_table)
} # end function
```

<br>

# Rank Sum Test

`rank_sum(dat)`
```{r}
#| label: wilcoxon-rs-test-base

#> dat: dataset with just rM/counts and grouping variable in 1st column
  #> grouping variable must be in 1st column, followed by count columns
rank_sum <- function(dat){
  
  # Arrange by grouping variable
  dat <- dat %>% arrange(!!sym(colnames(dat)[1]))
  
  # Create a matrix to store results with rows = the number of rM columns
  res_matrix <- matrix(ncol = 2, nrow = ncol(dat)-1,
                       dimnames = list(NULL, c("var", "p_value")))
  
  for (i in 1:nrow(res_matrix)){
    # Store data with tx group (1) and class column i (i+1)
    dat_col <- dat[,c(1,i+1)] 
    
    # Store class name
    res_matrix[i,1] <- colnames(dat_col)[2]
    
    # Skip any columns with all 0s
    if (all(dat_col[,2] == 0)) {
      next
    } 
    # Rank sum test and store in p-value column (2): class_i_counts ~ tx
    res_matrix[i,2] <-
      wilcox.test(dat_col[,2][[1]] ~ dat_col[,1][[1]],
                  data = dat_col)$p.value
  } # for i
  
  # Make output into a tibble
  res_matrix <- as_tibble(res_matrix) %>% 
    mutate(p_value = if_else(!is.nan(p_value), as.numeric(p_value), NA))
  # Return result matrix
  return(res_matrix)
  
} # end function
```

`rank_sum_longitudinal(dat, time_col, group_col, start_col)`
```{r}
#| label: ranksum-longitudinal

# Wilcoxon rank sum test for all timepoints in a study
  #> dat: dataset with all meta data before count columns 
  #> start_col: the index of first column with counts
  #> group_col: grouping column (tx)
  #> time_col: time (longitudinal) or measurment month/year
rank_sum_longitudinal <- function(dat, time_col, group_col, start_col){
  
  # Sort and store timepoints
  times <- sort(as.numeric(unique(as.vector(unlist(dat[,time_col])))))
  
  # Names of the classes
  rM_names <- colnames(dat)[start_col:ncol(dat)]
  # Name the grouping column 
  colnames(dat)[group_col] <- "group"
  # Name the time column
  colnames(dat)[time_col] <- "timepoint"
  
  # Tibble for storing results for all time points
  all_result <- tibble(timepoint = NA, var = NA, p_value = NA)
  
  for (t in 1:length(times)){
    # Filter for time t, remove all variables except counts and group column
    # Arrange by grouping variable
    dat_t <- dat %>% 
      filter(timepoint == times[t]) %>% 
      select(group, all_of(rM_names)) %>% 
      arrange(group)
    
    # Create a matrix to store results with rows = the number of rM columns
    res_matrix <- matrix(
      ncol = 3, nrow = length(rM_names), 
      dimnames = list(NULL, c("timepoint", "var", "p_value"))
    )
    
    for (i in 1:length(rM_names)){
      # Store data with tx group (1) and class column i (i+1)
      dat_col <- dat_t[,c(1,i+1)] 
      
      # Store timepoint
      res_matrix[i,1] <- times[t]
      # Store class name
      res_matrix[i,2] <- colnames(dat_col)[2]
      
      # Skip any columns with all 0s
      if (all(dat_col[,2] == 0)) {
        next
      } 
      # Rank sum test and store in p-value column (2): class_i_counts ~ tx
      res_matrix[i,3] <-
        wilcox.test(dat_col[,2][[1]] ~ dat_col[,1][[1]],
                    data = dat_col)$p.value
    } # for i
    
    # Bind to the storing tibble
    all_result <- all_result %>% 
      rbind( # Make output into a tibble
        as_tibble(res_matrix) %>% 
          mutate(
            timepoint = as.numeric(timepoint),
            p_value = if_else(!is.nan(p_value), as.numeric(p_value), NA))
      )
    
    # If first iteration of the loop, remove NA row used to initialize the tibble
    if (t == 1){
      all_result <- all_result[-1,]
    }
    
  } # for t
  
  # Return result matrix
  return(all_result)
  
} # end function

```



`rank_sum_minp(dat, time_col, group_col, start_col, B = 9999)`
```{r}
#| label: ranksum-minp

# Wilcoxon rank sum test for all timepoints in a study
# Westfall-Young Min-p adjustment for multiple comparisons
  #> dat: dataset with all meta data before count columns 
  #> B: number of permutations
  #> start_col: the index of first column with counts
  #> group_col: grouping column (tx)
  #> total_reads_col: column with total non-human read numbers for log transforming
  #> time_col: time (longitudinal) or measurment month/year
    #> if not a longitudinal study, use seperate datasets for pre.post intervention min-p calculation
rank_sum_minp <- function(dat, time_col, group_col, start_col, B = 9999){
  
  # Sort and store timepoints
  times <- sort(as.numeric(unique(as.vector(unlist(dat[,time_col])))))
  # Names of the classes
  rM_names <- colnames(dat)[start_col:ncol(dat)]
  # Make sure group column is named tx
  colnames(dat)[group_col] <- "tx"
  
  # Call function to compute observed pvalues
  observed_result_table <- rank_sum_longitudinal(dat, time_col = time_col, 
                                                 group_col = group_col,
                                                 start_col = start_col)
  # Store p-values
  observed_p_values <- observed_result_table$p_value
  
  # Extract observed minimum P-value
  observed_min_p <- min(observed_p_values, na.rm = TRUE)
  
  # Storage for permuted minimum P-values (numeric vector)
  min_perm_p_values <- numeric(B)
  
  #min_perm_p_values <- tibble(nrow = B, ncol = 2)
  
  for (b in 1:B) { # Permutation loop
    
    # Sample IDs without replacement
    perm_dat <- dat %>% 
      mutate(tx = sample(tx, length(tx), replace = FALSE))
    
    # Compute p-values from permuted data
    perm_p_result <- rank_sum_longitudinal(perm_dat, time_col = time_col,
                                           group_col = group_col, 
                                           start_col = start_col
    )
    # Record minimum P-value from vector of permuted p-values
    min_perm_p_values[b] <- min(perm_p_result$p_value, na.rm = TRUE)
    
  } # for b
  
  # Use this look to create the correct time labels
  # The rank sum function does all classes for each time individually, so need 18 of each timepoint sequentially
  time_var <- c()
  # Build a vector of times
  for (t in 1:length(times)){
    time_var <- append(time_var, rep(times[t], length(rM_names)))
  }
  
  # Adjust P-values
  m_padj <- tibble(
    timepoint = time_var,
    var = rep(rM_names, length(times)),
    minp_adj = rep(0)
  )
  for (i in 1:length(observed_p_values)) {
    # Proportion of permuted min p values less than or equal to the observed p-value i
    m_padj[i,3] <- mean(min_perm_p_values <= observed_p_values[i])
    
  } # for i
  
  padj_result_table <- observed_result_table %>% 
    left_join(m_padj, by = c("var", "timepoint"))
  
  # Return results
  return(padj_result_table)
  
} # end function


```

<br>


# Diversity


$$L_{\alpha} = \left( \sum^n_{i \ = \ 1} P_i^{\ \ \alpha} \right) ^{\frac{1}{1 \ - \ \alpha}}$$


`shannon(v)`

`simpson(v)`

```{r}
#| label: shannon-simpson

# Shannon: Calculate L1 Diversity for a vector of counts or normalized reads
shannon <- function(v){
  
  prob <- c()
  for (i in 1:length(v)){
    
    # Handle zeros
    if (v[i] != 0){
      # Genus proportion of set
      prob[i] <- v[i]/sum(v)
    } else{ next }
    
  } # end for
  
  # Remove NA
  prob <- prob[!is.na(prob)]
  
  if (is.null(prob)) {
    # Return 1 for shannon diversity if all values are 0 exp(0) = 1
    div <- 1
  } else {
    
    # calculate diversity
    div <- exp(-sum(prob*log(prob)))
    
  } # end else
  
  return(div)
  
} # end function



# Calculate L2 Diversity for a vector of counts or normalized reads
# Returns a vector with [1] simpson's index, and [2] inverse simpson 
simpson <- function(v){
  
  prob <- c()
  for (i in 1:length(v)){
    
    # Deal with 0 sum
    if (sum(v) == 0){
      prob[i] <- 0
    } else{
      
      # Genus proportion of set
      prob[i] <- v[i]/sum(v)
      
    } # end else
    
  } # end for loop
  
  # Simpson's index 
  si <- 1 - sum(prob^2)
  # Calculate inverse Simpson's index
  inv_si <- (sum(prob^2))^(-1)
  
  return(c(si, inv_si))
  
}

```



`calc_diversity(dat, start_col)`


```{r}
#| label: calc-diversity

# Calculate all diversity indices simultaneously
  #> Output table with treatment arm, shannon, simpson's index, and inverse simpson
  #> dat: Count data with treatment arm in column immediately before counts start
  #> start_col: Index of first count column 

calc_diversity <- function(dat, start_col){
  
  # Store just diversity variables as data
  data <- as.data.frame(dat[, start_col:ncol(dat)])
  
  # Store ID variables
  arm <- as.data.frame(dat[, start_col-1])
  
  # Create empty matrix to store values
  div_out <- matrix(nrow = nrow(arm), ncol = 3,
                    dimnames = list(NULL, c("shannon", "simpson", "inv_simpson"))
  )
  for (i in 1:nrow(arm)){
    
    # Pool data
    vi <- as.numeric(data[i,])
    # Calculate Shannon Diversity for row i
    l1 <- shannon(vi)
    l2 <- simpson(vi)
    
    # Store diversity
    div_out[i,1] <- l1    # Shannon
    div_out[i,2] <- l2[1] # SI
    div_out[i,3] <- l2[2] # inverse SI
    
  } # end for
  
  # Fill row i in 1st column with treatment arm
  div_out <- as_tibble(cbind(arm, as.data.frame(div_out)))
  
  return(div_out)
  
} # end function
```


`plot_diversity(dat, xlim, timepoint, groups, colors)`

```{r}
#| label: plot-diversity

# Plot diversity values from calc_diversity() output, requires `patchwork` loaded
  #> dat: Output table from calc_diversity()
  #> xlim: Manually set limits of x-axis (if you don't do this, values get truncated)
  #> timepoint: Character string for plot title (e.g. "baseline", "36 months"..)
  #> groups: Character vector for naming groups in the legend
  #> colors: Vector of colors matching groups

plot_diversity <- function(dat, xlim, timepoint, groups, colors){
  
  # Rename group column tx if it isn't already
  if(colnames(dat)[1] != "tx") colnames(dat)[1] <- "tx"
  
  p_shannon <- ggplot(data = dat) +
    geom_density(
      aes(shannon, fill = tx), 
      alpha = 0.5,
      color = FALSE
    ) + 
    scale_fill_manual(
      labels = groups,
      values = colors
    ) +
    scale_x_continuous(limits = xlim) +
    labs(
      title = str_c("Diversity", timepoint, sep = " "),
      x = "Shannon's Diversity (effective number)",
      y = "Density"
    ) +
    theme_light() +
    theme(
      legend.title = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
      # aspect.ratio = 1
    )
  
  p_inv_si <- ggplot(data = dat) +
    geom_density(
      aes(inv_simpson, fill = tx), 
      alpha = 0.5,
      color = FALSE
    ) + 
    scale_fill_manual(
      labels = groups, 
      values = colors
    ) +
    scale_x_continuous(limits = xlim) +
    labs(
      x = "Inverse Simpson's Diversity (effective number)",
      y = "Density"
    ) +
    theme_light() +
    theme(
      legend.title = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank()
      # aspect.ratio = 1
    )
  
  p_combined <- p_shannon + p_inv_si + plot_layout(guides = "collect")
  
  return(p_combined)
  
} # end function

```


<br>

# PERMANOVA

`fit_permanova(dat, start_col, mthd, perm = 9999)`

```{r}
#| label: fit-permanova

# Combines runs PERMANOVA, comparing groups at a single timepoint
  #> dat: count data with time variable and group column preceding count columns
  #> start_col: first column with counts
  #> mthd: distance measure for `vegan::adonis2()`
  #> perm: number of permutations

fit_permanova <- function(dat, start_col, mthd, perm = 9999){
  
  # Filter data
  arm_dat <- dat[,c(1:start_col-1)]
  # Diversity data as matrix
  count_dat <- as.matrix(dat[,-c(1:start_col-1)])
  
  # Treatment arm
  arm <- as.matrix(arm_dat[,start_col-1])
  
  # PERMANOVA model
  permanova_out <- vegan::adonis2(
    count_dat ~ arm,
    #data = arm_dat,
    method = mthd,
    permutations = perm
  )
  return(permanova_out)
  
} # end function

```

<br>

# PCoA

`pcoa(dat, start_col, method)`

```{r}
#| label: pcoa

# Function returning Eigenvectors, Eigenvalues, and Centroids from PCoA
  #> dat: count data with time variable and group column preceding count columns
  #> start_col: first column with counts
  #> method: distance measure for vegan distance matrix (takes "l1" or "l2")

pcoa <- function(dat, start_col, method){
  
  # Sort by group
  dat <- arrange(dat, dat[,start_col-1][[1]])
  
  # Store just diversity variables as data
  data <- dat[, start_col:ncol(dat)]
  
  # Store ID variables
  arm <- dat[, 1:start_col-1]
  
  # Compute distance matrix using selected norm as method and store as dis
  if (method == "l1"){
    dist_m <- vegdist(data, method = "manhattan")
  } else if (method == "l2"){
    dist_m <- vegdist(data, method = "euclidean")
  } else {
    stop("Invalid method input: choose 'l1' or 'l2'")
  }
  
  # PCoA with distance matrix
  pcoa_mod <- betadisper(d = dist_m, group = arm[,start_col-1][[1]])
  
  # Site points in multivariate space
  vectors <- data.frame(
    group = pcoa_mod$group,
    data.frame(pcoa_mod$vectors)
  )
  
  # Name each PC
  names(vectors) <- "group"
  for (i in 2:ncol(vectors)){
    names(vectors)[i] <- str_c("v_PCoA", as.numeric(i-1))
  }
  
  # Centroids in multivariate space  
  centroids <- data.frame(
    group = rownames(pcoa_mod$centroids),
    data.frame(pcoa_mod$centroids)
  )
  
  # Eigenvalues
  eigs <- as.data.frame(as.numeric(pcoa_mod$eig))
  names(eigs) <- "value"
  
  out <- list(centroids, vectors, eigs)
  names(out) <- c("centroids", "vectors", "eig")
  
  # Return output eigenvalues and pc1/pc2 eigenvectors 
  return(out)
  
} # end function

```


`plot_pcoa(vectors, centroids, label, colors = NULL, title_text = NULL, subtitle_text = NULL)`

```{r}
#| label: plot-pcoa

# Plot Centroids from PCoA1 and PCoA2
  #> vectors: Eigenvectors from pcoa() output
  #> centroids: Centroids from pcoa() output
  #> label: labels for groups in the legend
  #> colors: option to manually set group colors
  #> title_text: optional plot title
  #> subtitle_text: optional plot subtitle

plot_pcoa <- function(vectors, centroids, label, colors = NULL,
                      title_text = NULL, 
                      subtitle_text = NULL){ 
  
  # Eigenvectors and Centroids for PC1 and PC2 
  v1_2 <- as_tibble(vectors[,1:3])
  c1_2 <- as_tibble(centroids[,1:3]) %>% 
    select(group, c_PCoA1 = PCoA1, c_PCoA2 = PCoA2)
  
  # Values to plot lines from segments to points in tibble format
  for_seg <- v1_2 %>% 
    group_by(group) %>% 
    left_join(c1_2, by = "group") %>% 
    # add in a variable labeling row numbers by group
    mutate(point = row_number())
  
  # PCoA 1 and 2
  plot_1_2 <- 
    ggplot() + 
    stat_ellipse(
      data = for_seg,
      aes(x = v_PCoA1, y = v_PCoA2, fill = group),
      geom = "polygon",
      alpha = 0.175
    ) +
    geom_segment(
      data = for_seg,
      aes(x = v_PCoA1, xend = c_PCoA1,
          y = v_PCoA2, yend = c_PCoA2,
          color = group),
      alpha = 0.30
    ) +
    # centroids
    geom_point(
      data = for_seg, # group and first two PC
      aes(x = c_PCoA1, y = c_PCoA2, color = group),
      size = 5,
      # shape = 18
    ) +
    # points
    geom_point(
      data = for_seg,
      aes(x = v_PCoA1, y = v_PCoA2, color = group), 
      size = 2
    ) +
    labs(
      title = title_text,
      subtitle = subtitle_text,
      x = "PCoA1",
      y = "PCoA2"
    ) +
    theme_light() +
    theme(
      legend.title = element_blank(),
      panel.grid.major = element_blank(),
      panel.grid.minor = element_blank(),
      aspect.ratio = 1)
  
  if (is.null(colors)){
    
    return(plot_1_2)
    
  } else {
    
    plot_1_2 <- plot_1_2 +     
      scale_color_manual(
        labels = label,
        values = colors
      ) +
      scale_fill_manual(
        labels = label,
        values = colors
      )
    return(plot_1_2)
    
  } # end else
  
} # end function

```

<br>



# Correlation


`corr_fdr(dat, method = "spearman", p.adjust.method = "fdr", nonzero_thres = round(0.1*nrow(dat)), var_lab = NULL, digits = NULL)`

```{r}
#| label: corr-test-padj

corr_fdr <- function(dat, method = "spearman", p.adjust.method = "fdr",
                     nonzero_thres = round(0.1*nrow(dat)),
                     var_lab = NULL, digits = NULL){
  
  # if no var_lab provided, use a generic one
  if(is.null(var_lab)) var_lab <- "var"
  
  # First: Filter out columns with too many zeros
  # Vector to store names of columns to remove
  remove_cols <- c()
  # Convenience vector to track the non-zero values in each column
  for (i in 1:ncol(dat)){
    # Create a counter for non-zero values in a column
    non_zero <- 0
    for (j in 1:nrow(dat[,i])){
      if (dat[j,i] > 0){
        non_zero <- non_zero+1
      } else{next}
    } # end for j
    # Store column names with fewer than threshold for non-zero values
    if (non_zero <= nonzero_thres){
      remove_cols[i] <- colnames(dat)[i]
    } else{next}
  } # end for i
  # Take out NA values
  remove_cols <- remove_cols[!is.na(remove_cols)]
  # Remove all zero columns
  dat_clean <- dat %>%
    select(-all_of(remove_cols))
  
  # Second: get spearman correlation coefficients
  corr_coef <- as.data.frame(cor(dat_clean, method = method))
  # Get just the top triangle
  corr_coef_top <- matrix(ncol = ncol(corr_coef), nrow = nrow(corr_coef))
  for (i in 1:ncol(corr_coef)){
    # vector to store values before 1
    vals <- c()
    # counter
    j <- 0
    while (j+1 != i){ # stop loop at diagonal
      vals[j+1] <- corr_coef[j+1,i]
      j <- j + 1
    } # end while
    # Append on NA to create a vector the same length as nrow(corr_coef)
    corr_coef_top[,i] <- append(vals, rep(NA, length(corr_coef[,i]) - j))
  } # end for i
  # Rename rows and columns
  colnames(corr_coef_top) <- rownames(corr_coef_top) <- colnames(corr_coef)
  
  # Validate dimensions of correlation matrix
  if (any(dim(corr_coef) != c(ncol(dat_clean), ncol(dat_clean))) | 
      any(dim(corr_coef_top) != c(ncol(dat_clean), ncol(dat_clean)))){
    break
  } # end if
  
  # Third: correlation p value with cor.test()
  corr_pval <- matrix(nrow = ncol(dat_clean), ncol = ncol(dat_clean))
  for (i in 1:ncol(dat_clean)){
    for (j in 1:ncol(dat_clean)){
      
      corr_pval[i,j] <- cor.test(dat_clean[[i]], dat_clean[[j]],
                                 method = method, exact = FALSE)$p.value
    } # end j
  } # end i
  # Get just the top triangle
  corr_p_top <- matrix(ncol = ncol(corr_pval), 
                       nrow = nrow(corr_pval))
  for (i in 1:ncol(corr_pval)){
    # vector to store values before 1
    vals <- c()
    # counter
    j <- 0
    while (j+1 != i){ # stop loop at diagonal
      vals[j+1] <- corr_pval[j+1,i]
      j <- j + 1
    } # end while
    # append on NA
    corr_p_top[,i] <- append(vals, rep(NA, length(corr_pval[,i]) - j))
  } # end for i
  # Rename rows and columns
  colnames(corr_p_top) <- rownames(corr_p_top) <- colnames(corr_coef)
  # Validate dimensions of p matrix
  if (any(dim(corr_pval) != c(ncol(dat_clean), ncol(dat_clean))) | 
      any(dim(corr_p_top) != c(ncol(dat_clean), ncol(dat_clean)))){
    break
  } # end if
  
  # Fourth: FDR
  # Convert matrix values to vector
  vector_vals <- as.vector(corr_p_top)
  # NA positions
  na_positions <- is.na(vector_vals)
  # Remove NA values
  vector_p_no_na <- vector_vals[!na_positions]
  
  if (p.adjust.method == "fdr"){
    # FDR BH
    vector_padj <- p.adjust(vector_p_no_na, method = "fdr")
  } else{
    if(p.adjust.method == "bonferroni"){
      # Bonferroni correction
      vector_padj <- p.adjust(vector_p_no_na, method = "bonferroni")
    } else{
      if (p.adjust.method == "holm"){
        vector_padj <- p.adjust(vector_p_no_na, method = "holm")
      } else{
        stop("invalid input for p.adjust.method") 
      } # ifelse 3
    } # ifelse 2
  } # ifelse 1
  
  # Re-insert NA at original positions
  vector_vals[!na_positions] <- vector_padj
  # Coerce vector back into matrix
  corr_padjust_top <- matrix(vector_vals, nrow = nrow(corr_coef))
  # Add back in arg_group names
  colnames(corr_padjust_top) <- rownames(corr_padjust_top) <- colnames(corr_coef)
  
  # Validate dimensions of padj matrix
  if (any(dim(corr_padjust_top) != c(ncol(dat_clean), ncol(dat_clean)))) break
  
  # Fifth: Extract name pairs of arg_groups with padj values less than 0.05
  # Matrix to store 5 variables (gene names, cor coef, raw p-value, and q-value)
  pairs <- matrix(nrow = 1, ncol = 5, 
                  dimnames = list(
                    NULL, 
                    c(str_c(var_lab, "1"), str_c(var_lab, "2"), 
                      "cor", "pval", "padj")))
  
    # Skip NA
    for (i in 1:ncol(corr_padjust_top)){
      for(j in 1:nrow(corr_padjust_top)){
        # If it isnt NA
        if(!is.na(corr_padjust_top[j,i])){
          pairs <- rbind(
            pairs, c(rownames(corr_padjust_top)[i], 
                     colnames(corr_padjust_top)[j], 
                     # Add in the coefficient from the other matrix 
                     corr_coef[j,i],  # corr coef
                     # Raw p-value
                     corr_p_top[j,i],      # raw p
                     # padj for selected method
                     corr_padjust_top[j,i] # padj col
            )
          )
          # Convert to tibble and make variables numeric
          pairs <- as_tibble(pairs) %>% 
            mutate(
              cor = as.numeric(cor),
              pval = as.numeric(pval), 
              padj = as.numeric(padj)
            )
        } else{ next } # end ifelse
      } # end j
    } # end i
  
  # Remove first NA row 
  pairs <- pairs[-1,]
  
  if(!is.null(digits)){
    pairs <- as_tibble(pairs) %>% 
      mutate(
        cor = signif(as.numeric(cor), digits),
        pval = signif(as.numeric(pval), digits),
        padj = signif(as.numeric(padj), digits)
      )
  }
  
  # Return a list with the correlation, pvalue, fdr matrices, and a summary table
  return(list("correlation_coefficients" = corr_coef_top, 
              "pvalue_matrix" = corr_p_top,
              "qvalue_matrix" = corr_padjust_top, 
              "pairwise_comparison" = pairs,
              "removed_cols" = remove_cols)
  )
  
  
} # end function
```







<br>

# Longitudinal Results


`combine_result_table(list_results, grp_name)`

```{r}
#| label: longitudinal-combine-table-all-result

# Combines all outputs for different timepoints into a single table
  #> Use this function over combine_pval_table()
  #> grp_name: the name of the test group to pull out (i.e. "MLS")
combine_result_table <- function(list_results, grp_name){
  
  # Rename first column something generic
  colnames(list_results[[1]])[1] <- "var"
  
  tib_base <- list_results[[1]] %>% 
    filter(var == grp_name) %>% 
    mutate(timepoint = 0) %>% 
    relocate(timepoint, .after = "var")
  
  
  for (i in 2:length(list_results)){
    
    # Rename first column something generic
    colnames(list_results[[i]])[1] <- "var"
    
    tib_time_i <- list_results[[i]] %>% 
      filter(var == grp_name) %>% 
      mutate(timepoint = i-1) %>% 
      relocate(timepoint, .after = "var")
    
    tib_base <- rbind(tib_base, tib_time_i)
    
  } # end for
  
  return(tib_base)
  
} # end function
```



`combine_pval_table(list_results, var_col, var_name, bind = FALSE)`

```{r}
#| label: longitudinal-combine-just-pval-table

# Combines p-value outputs for different timepoints in a single table
  #> list_results: fit_permanova() output for each timepoint collected in a list
  #> var_col: the column of the result variable to be represented (e.g. column with `padj`, etc)
  #> var_name: name of the variable in var_col
  #> time_name: manually set the name of timepoints
  #> bind: if no key variable for joining exists, use bind = TRUE to bind columns instead of joining

combine_pval_table <- function(list_results, var_col, var_name,
                          time_name = NULL, bind = FALSE){
  
  # if time names are not specified use indices 0-n
  if (is.null(time_name)){
    
    # Loop to rename and store just variable of interest
    new_list <- list()
    for (i in 1:length(list_results)){
      
      # tibble with column 1 and result variable
      tib_i <- list_results[[i]][, c(1, var_col)]
      # Change the name of the variable to reflect timepoint
      colnames(tib_i)[2] <- str_c(var_name, as.character(i-1), sep = "_")
      # Store in the new list
      new_list[[i]] <- tib_i
      
    } # end for
    
  } else {
    
    # Loop to rename and store just variable of interest
    new_list <- list()
    for (i in 1:length(list_results)){
      
      # tibble with column 1 and result variable
      tib_i <- list_results[[i]][, c(1, var_col)]
      # Time name
      timep <- time_name[i]
      # Change the name of the variable to reflect timepoint
      colnames(tib_i)[2] <- str_c(var_name, timep, sep = "_")
      # Store in the new list
      new_list[[i]] <- tib_i
      
    } # end for
    
  } # end else
  
  if (bind == FALSE){ # Left Joining when a key variable exists
    
    # Starting place for creating the full table
    tib_full <- new_list[[1]]
    # Store the name of the key variable to join tibbles with
    join_var <- colnames(tib_full)[1]
    
    for(j in 2:length(new_list)){
      
      tib_j <- new_list[[j]]
      
      # Join columns
      tib_full <- tib_full %>% 
        left_join(tib_j, by = join_var)
      
    } # end for
    
  } else{ # If there is no key variable to join, then bind 
    
    # Starting place for creating the full table
    tib_full <- new_list[[1]]
    
    for(j in 2:length(new_list)){
      
      tib_j <- new_list[[j]][,2]
      
      # Bind columns
      tib_full <- as_tibble(tib_full %>% cbind(tib_j))
    } # end for
    
  } # end else
  
  return(tib_full)
  
} # end function

```



